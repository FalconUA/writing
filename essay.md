Can computers think?
====================

The question whether an artificially created entity can have a thought has been asked by greatest philosophical minds for thousands of years. According to Hubert Dreyfus (1979), the study on intelligence and artificial mind might be traced back to around 450 B.C., when Socrates asked if there is an algorithm that can govern a rational mind. From the next millennia of developments in philosophy, theories of reasoning and learning have emerged, along with the viewpoint that the mind can be constituted by the operation of a physical system (the materialism approach). Modern mathematics came up with the theories of logic, probability, decision-making, control, optimization, and computation. Psychology provides us with the tools to investigate the human mind. Finally, state of the art nanoengineering along with computer science and neuroscience provides us with the tools and models to imitate the living brain more precisely than ever. 

Naturally emerges the question if we can create a computer that is able to think, given the tools and knowledge that we already have now or in the future? This essay will approach this question from predominantly Artificial Intelligence (AI) point of view. Later, the importance of this question and its implications in AI-based technology in the future will be discussed.

To avoid ambiguity in answering the question above, it is important to define what we mean by "thinking". The problem of most of the works related to philosophy of thoughts is that they do not give a clear definition to the object they are discussing about.

In very narrow terms, "thinking" can be understood as the process of deriving logical conclusions from the given facts and prior knowledge. Russel (1872-1970), governing the principle of induction, introduced the Logical Positivism -- a doctrine which holds that all knowledge can be characterized by logical theories applied on observation sentences that corresponds to sensory input. Following the logicism approach, Newell and Simon (1957) presented a reasoning program, the General Problem Solver, that was able to solve any problem that can be expressed as a set of well-formed formulas. Obviously, in narrow terms, a computer can definitely think.

Leaving aside the issue that logical inference can be co-NP-complete for propositional logic and can be inrecognizable for first-order logic according to Godel's incompleteness theorem, this logistics approach does not satisfy our intuitive understanding of the thinking process. 

Although almost each philosophycal work has its own definition of the thinking process, all of them agrees that thoughts are a mental cognitive experience can only exist in conscious entities. Some of them suggests that thoughts and consciousness are the same thing. Alexander (2005) suggested 5 basic features of conscious model, which appeared important throught introspection:
1. Perception of oneself in an 'out-there' world
2. Imagination of past events and fiction
3. Inner and outer attention
4. Volition and planning
5. Emotion
The author also suggests that consciousness is a composition of the above sensations. The sensation of future is explicitly included as a result of imagination and ability to plan. It also includes the ability to think, as the consequence of imagination of fiction events and the sensation of future. Thus, by defining the process of thinking as a product of conscious experience, the question of whether a computer can think can be answered by asking a more general question: is it possible to create consciousness in a computer?

Alexander (2005) suggested how these 5 properties can be created using a system of connected neurons. Searle (1992) in his book The Rediscovery of the Mind described that consciousness is an emergent property of appropriately arranged systems of neurons. Therefore, it might be possible for current state-of-the-art Recurrent Neural Networks to be conscious, but if it is, its conscious experience will be different to any living things. Let's take a closer look at this state-of-the-art instrument of AI which resembles the human brain, its development, importance, and the main problem that it have.

The revolutionaly paper by Kryzhevski et. al. (2012) has brought a rapid rise to the fields of Neural Networks and Deep Learning. Deep Neural Networks (DNN) were far more effective at recognising the patterns in data comparing to statistical approaches. Public datasets such as ImageNet (Deng et al. 2009) has made AI more available for individual researches, institutions and small companies, which resulted such a fast development of this field as we see today. 

Despite the unprecedented accuracy of DNNs on variety of pattern-recognition tasks, they are vulnerable to certain types of input data. For instance, Nguyen et al. (2015) showed that there is a way to generate a white noise that DNNs believe to be recognisable object with 99% confidence. Goodfellow et al. (2015) demonstrated that imperceptible to the human eye changes in an image can cause the DNN to switch from classifying it as class A to class B. This raises a lot of security issues with AI-based automation systems, especially in the technologies directly related to human safety.

These works shows that, although the field of Deep Learning is trying to mimic the structure and behaviour of human brain, there is a fundamental difference between the way DNNs and humans processes the input data from sensors. At current state, DNNs does not have semantic understanding of data (e.g. visual objects). A full, par-human level of understanding of semantic information, ultimately, requires a certain sort of consciousness. Since thoughts are a product of consciousness, for a system to think the same way humans or animals do, it have to understand the semantic of its surrounding.

To sidestep the philosophycal problem of consciousness, many attemts have been done in order to explicitly represent semantic understanding within the AI system. Chomsky (1957) suggested a linguistics approach, according to which semantic knowledge can be represented by syntactic structures. Recently, complementing the concept of linguistics with pattern recognition, a new image database called Visual Genome (Krishna et al. 2016) was created, attempting to provide a deeper understanding of visual data, from pixel-level patterns to relationship between objects.

Although we does not have a formal theory that can describe the mind or conscious experience yet, it is clear that in order to create a machine that can think as a human, we have to solve the main problem of vision: the problem of implicit semantic understanding.



Alexander I. (2005), Machine consciousness. S. Laureys (Ed.), Progress in Brain Research, 150(99-108).
