Can computers think?
====================

The question whether an artificially created entity can have a thought has been asked by greatest philosophical minds for thousands of years. According to Hubert Dreyfus (1979), the study on intelligence and artificial mind might be traced back to around 450 B.C., when Socrates asked if there is an algorithm that can govern a rational mind. From the next millennia of developments in philosophy, theories of reasoning and learning have emerged, along with the viewpoint that the mind can be constituted by the operation of a physical system (the materialism approach). Modern mathematics came up with the theories of logic, probability, decision-making, control, optimization, and computation. Psychology provides us with the tools to investigate the human mind. Finally, state of the art nanoengineering along with computer science and neuroscience provides us with the tools and models to imitate the living brain more precisely than ever. 

This brings up the question whether we can create a computer that is able to think, given the tools and knowledge that we already have now or in the future? This essay will approach this question from predominantly Artificial Intelligence (AI) point of view. Later, the importance of this question in AI-based technology in the future will be discussed.

To avoid ambiguity in answering the question above, it is important to define what we mean by "thinking". The problem of most of the works related to the philosophy of thoughts is that they do not give a clear definition to the object they are discussing.

In very narrow terms, "thinking" can be understood as the process of deriving logical conclusions from the given facts and prior knowledge. Russel (1872-1970), governing the principle of induction, introduced the Logical Positivism -- a doctrine which holds that all knowledge can be characterized by logical theories applied on observation sentences that correspond to sensory input. Following the logicism approach, Newell and Simon (1959) presented a reasoning program, the General Problem Solver, that was able to solve any problem that can be expressed as a set of well-formed formulas. Obviously, in narrow terms, a computer can definitely think.

Leaving aside the issue that logical inference can be co-NP-complete for propositional logic and can be unrecognizable for first-order logic according to Godel's incompleteness theorem, this logistics approach does not satisfy our intuitive understanding of the thinking process.

Although almost each philosophical work has its own definition of the thinking process, all of them agrees that thoughts are a mental cognitive experience can only exist in conscious entities. Some of them suggest that thoughts and consciousness are the same things. Alexander (2005) suggested 5 basic features of a conscious model, which appeared important through introspection:
1. Perception of oneself in an "out-there" world
2. Imagination of past events and fiction
3. Inner and outer attention
4. Volition and planning
5. Emotion
The author also suggests that consciousness is a composition of the above sensations. The sensation of future is explicitly included as a result of imagination and ability to plan. It also includes the ability to think, as the product of imagination of fictional events and the sensation of future. Thus, by defining the process of thinking as a product of conscious experience, the question whether a computer can think can be answered by asking a more general question: is it possible to create consciousness in a computer?

Alexander (2005) suggested how these 5 properties can be created using a system of connected neurons. Searle (1992) in his book The Rediscovery of the Mind described that consciousness is an emergent property of appropriately arranged systems of neurons. Therefore, it might be possible for state-of-the-art Recurrent Neural Networks to be conscious, but if it is, its conscious experience will be different to any living things. Let's take a closer look at this state-of-the-art instrument of AI which resembles the human brain, its development, importance, and the main problem that it has.

The revolutionary paper by Kryzhevski et. al. (2012) has brought a rapid rise to the fields of Neural Networks and Deep Learning. Deep Neural Networks (DNN) were far more effective at recognizing the patterns in data comparing to statistical approaches. Public datasets such as ImageNet (Deng et al. 2009) has made AI more available for individual researchers, institutions, and small companies, which caused such a fast development of this field as we see today.

Despite the unprecedented accuracy of DNNs on the variety of pattern-recognition tasks, they are vulnerable to certain types of input data. For instance, Nguyen et al. (2015) showed that there is a way to generate a white noise that DNNs believe to be the recognisable object with 99% confidence. Goodfellow et al. (2015) demonstrated that imperceptible to the human eye changes in an image can cause the DNN to switch from classifying it as class A to class B. This raises a lot of security issues with AI-based automation systems, especially in the technologies directly related to human safety.

These works show that, although the field of Deep Learning is trying to mimic the structure and behavior of the human brain, there is a fundamental difference between the way DNNs and humans process the input data from sensors. At the current state, DNNs does not have a semantic understanding of data (e.g. visual objects). A full, par-human level of understanding of semantic information, ultimately, requires a certain sort of consciousness. Since thoughts are a product of consciousness, for a system to think the same way humans or animals do, it has to understand the semantic of its surrounding.

To sidestep the philosophical problem of consciousness, many attempts have been done in order to explicitly represent semantic understanding within the AI system. Chomsky (1957) suggested a linguistics approach, according to which semantic knowledge can be represented by syntactic structures. Recently, complementing the concept of linguistics with pattern recognition, a new image database called Visual Genome (Krishna et al. 2016) was created, attempting to provide a deeper understanding of visual data, from pixel-level patterns to relationship between objects.

Although we do not have a formal theory that can describe the mind or conscious experience yet, it is clear that in order to create a machine that can think as a human, we have to solve the main problem of vision: the problem of implicit semantic understanding.



Alexander, I. (2005). Machine consciousness. S. Laureys (Ed.), Progress in Brain Research, 150(99-108).

Chomsky, N. (1957). Syntactic Structures. Mouton, The Hague and Paris.

Dreyfus, H. L. (1979). What computers can't do: The limits of Artificial Intelligence. Harper and Row, New York, revised edition.

Goodfellow, I. J.; Shlens, J.; Szegedy, C. (2015) Explaining and harnessing adversarial examples. ICLR 2015

Krishna, R.; Zhu, Y.; Groth, O.; Johnson, J.; Hata, K.; Kravitz, J.;
Chen, S.; Kalantidis, Y.; Li, L.-J.; Shamma, D. A.; Bernstein, M.; Fei-Fei, L. (2016). Visual genome: Connecting language and vision using crowdsourced dense image annotations. arXiv:1602.07332

Kryzhevski, A.; Sutskever, I.; Hinton, G. E. (2012). Image classification with deep convolutional neural networks. In NIPS, pp. 1097-1105.

Newell, A.; Shaw, J.C.; Simon, H.A. (1959). Report on a general problem-solving program. Proceedings of the International Conference on Information Processing, pp. 256-264.

Nguyen A.; Yosinski J.; Clune J. (2015) Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images. In Computer Vision and Pattern Recognition (CVPR '15), IEEE

Searle, J. R. (1992). The rediscovery of the mind. Cambridge, Mass: MIT Press.


