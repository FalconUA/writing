Can computers think?
====================

The question of whether an artificially created entity can have a thought has been asked by greatest philosophycal minds for thousands of years. According to Hubert Dreyfus (1979), the study on intelligence and artificial mind might be traced back to around 450 B.C., when Socrates asked if there is an algorithm that can govern a rational mind. From the next millenia of developments in philosophy, theories of reasoning and learning have emerged, along with the viewpoint that the mind can be constituted by the operation of a physical system. Modern mathematics came up with the theories of logic, probability, decision making, control, optimization and computation. Psychology provides us with the tools to investigate the human mind. Finally, state of the art nanoengineering along with computer science and neuroscience provides us with the tools and models to imitate the living brain more precisely than ever.

To avoid ambiguity in answering the question of this essay, it is important to define what we mean by "thinking". The problem of most of the works related to philosophy of thoughts is that they do not give a clear definition to the object they are discussing about.

In very narrow terms, "thinking" can be understood as the process of deriving logical conclusions from the given facts and prior knowledge. Russel (1872-1970), governing the principle of induction, introduced the Logical Positivism -- a doctrine which holds that all knowledge can be characterized by logical theories applied on observation sentences that corresponds to sensory input. Following the logicism approach, Newell and Simon (1957) presented a reasoning program, the General Problem Solver, that was able to solve any problem that can be expressed as a set of well-formed formulas. Later, Gelernter (1959) constructed the Geometry Theorem Prover, which can prove theorems using explicitly represented axioms. Obviously, in narrow terms, a computer can definitely think.

Leaving aside the issue that logical inference can be co-NP-complete for propositional logic, or can be inrecognizable for first-order logic according to Godel's incompleteness theorem, this logistics approach does not satisfy our intuitive understanding of the thinking process. 

In broader terms, the process of thinking can be viewed as a consequence of consciousness. Alexander (2005) suggested five basic features of conscious model, which appear important throught introspection. 

The revolutionaly paper by Kryzhevski et. al. (2012), which Dr. Fei-Fei Li described as a "defining moment of the rennaisance of Deep Learning", has brought a rapid rise to the fields of Neural Networks and Deep Learning. Deep Neural Networks (DNN) were so effective at recognising the patterns in data comparing to statistical and linear optimization approaches and achieved such a huge success in different tasks, that it faded out the whole field of traditional Machine Learning. Public datasets such as ImageNet (Deng et al. 2009) and MNIST (LeCun, 1999) has made AI more available for individual researches, institutions and small companies, which resulted such a fast development of this field as we see today. 

Let's take a closer look at this state-of-the-art instrument of AI which resembles the visual cortex of human brain and the main problem that it have. Despite the unprecedented accuracy of DNNs on variety of pattern-recognition tasks, they are vulnerable to certain types of input data. For instance, Nguyen et al. (2015) showed that there is a way to generate a white noise that state-of-the-art DNNs believe to be recognisable object with 99% confidence. Goodfellow et al. (2015) demonstrated a method to change change the image of one class almost imperceptibly to the human eye in such a way that the DNN suddenly classifies it as any other class of choice, and argued that the primary cause of DNNs vulnerability is their linear nature. This raises a lot of security questions with AI-based automation systems, especially in the technologies directly related to human safety.

These works also shows that, although the field of Deep Learning is trying to mimic the structure and behaviour of human brain, there is a fundamental difference between the way DNNs and humans processes the input data from sensors. At current state, DNNs are, in essence, just an mathematical optimization tool. It does not have semantic understanding of data (e.g. visual objects). A full, par-human level of understanding of semantic information, ultimately, requires a certain sort of consciousness.

To sidestep the philosophycal problem of consciousness, many attemts have been done in order to explicitly represent semantic understanding within the AI system. Chomsky (1957) suggested a linguistics approach, according to which semantic knowledge can be represented by syntactic structures. Recently, complementing the concept of linguistics with pattern recognition, a new image database called Visual Genome (Krishna et al. 2016) was created, attempting to provide a deeper understanding of visual data, from pixel-level patterns to relationship between objects.


Alexander I. (2005), Machine consciousness. S. Laureys (Ed.), Progress in Brain Research, 150(99-108).
